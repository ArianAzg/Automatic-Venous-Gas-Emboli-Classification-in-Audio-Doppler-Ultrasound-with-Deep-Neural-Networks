{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e23754",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101724dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import random\n",
    "import math\n",
    "import struct\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "import keras\n",
    "from IPython.core.display import display, HTML\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf53bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebe74a",
   "metadata": {},
   "source": [
    "# Check for available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111cba0",
   "metadata": {},
   "source": [
    "# Visualizing a random test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d94eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"{}/Train/0/syntheticDopplerAudioCombined_0_892425.wav\".format(DATA_DIR)\n",
    "\n",
    "with wave.open(test_file, 'rb') as f:\n",
    "    params = f.getparams()\n",
    "    nchannels, sampwidth, framerate, nframes = params[:4] #Parameters of selected audio file\n",
    "    print(nchannels, sampwidth, framerate, nframes)\n",
    "    strData = f.readframes(nframes) \n",
    "\n",
    "waveData = np.frombuffer(strData, dtype=np.int16)        #Binary to wave conversion\n",
    "waveData_norm = waveData*1.0/(np.max(np.abs(waveData))) #Wave data\n",
    "\n",
    "n = 0.03*np.random.normal(0, 1, waveData_norm.shape)  #Noise to the selected data\n",
    "waveData_norm = n + waveData_norm         #Adding noise to waveform\n",
    "\n",
    "time = np.arange(0,nframes)*(1.0/framerate)\n",
    "plt.plot(time, waveData_norm)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Single channel wavedata\")\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05dd2a",
   "metadata": {},
   "source": [
    "# Organizing the data into train, valiadation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5180041",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAME = 40000 #Length of input frame for 8kHz data\n",
    "LABELS = ['0', '1', '2', '3', '4'] #Target classes for classification task\n",
    "\n",
    "N_CLASS = len(LABELS) #Number of classes for classification task\n",
    "\n",
    "train_files, test_files, val_files = [], [], []\n",
    "\n",
    "#####\n",
    "# \"Path_to_data\" in all data paths must be correctly set \n",
    "#####\n",
    "\n",
    "DATA_DIR_Train = r'C:\\Users\\Path_to_data\\DopplerDataset\\Shuffled_data\\Train'\n",
    "DATA_DIR_Test = r'C:\\Users\\Path_to_data\\DopplerDataset\\Shuffled_data\\Test'\n",
    "DATA_DIR_Validation = r'C:\\Users\\Path_to_data\\DopplerDataset\\Shuffled_data\\Validation'\n",
    "\n",
    "## Training Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train)):\n",
    "    train_files.append(f)   \n",
    "\n",
    "## Validation Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation)):\n",
    "    val_files.append(f)\n",
    "    \n",
    "## Test Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test)):\n",
    "    test_files.append(f) \n",
    "\n",
    "## Suffling the Training and Validation sets to remove bias in dataset\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2652d6a",
   "metadata": {},
   "source": [
    "# Reading and normalizing wave data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378abe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(w):\n",
    "    noise = 0.03*np.random.normal(0, 1, w.shape)\n",
    "    w = noise + w\n",
    "    return w\n",
    "\n",
    "## Functions to read all the data in Train, Validation, and Test sets and do normalization\n",
    "\n",
    "def get_wave_norm(file):\n",
    "    with wave.open(file, 'rb') as f:\n",
    "        params = f.getparams()\n",
    "        nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "        data = f.readframes(nframes)\n",
    "    data = np.frombuffer(data, dtype=np.int16)\n",
    "    data = data/(np.max(np.abs(data)))\n",
    "    return data\n",
    "\n",
    "def get_wave(file):\n",
    "    data = get_wave_norm(file)\n",
    "    wave_data = np.zeros((MAX_FRAME, ))\n",
    "    wave_len = min(MAX_FRAME, data.shape[0])\n",
    "    wave_data[:wave_len] = data[:wave_len]\n",
    "    return np.expand_dims(wave_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13569f3c",
   "metadata": {},
   "source": [
    "# Constructing the input and target: Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the input and target data - Training\n",
    "x_train = np.zeros((L, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_train = np.zeros((L, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L):\n",
    "    f_train = train_files[i]\n",
    "    x_train[i] = get_wave(f_train)\n",
    "    label = f_train[len(DATA_DIR_Train) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_train[i, label_idx] = 1\n",
    "     \n",
    "# Populating the input and target data - Validation\n",
    "L_val = len(val_files)\n",
    "x_val = np.zeros((L_val, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_val = np.zeros((L_val, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_val):\n",
    "    f_val = val_files[i]\n",
    "    x_val[i] = get_wave(f_val)\n",
    "    label = f_val[len(DATA_DIR_Validation) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_val[i, label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2c959",
   "metadata": {},
   "source": [
    "# Dataset balance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [1, 0, 0, 0, 0]\n",
    "a2 = [0, 1, 0, 0, 0]\n",
    "a3 = [0, 0, 1, 0, 0]\n",
    "a4 = [0, 0, 0, 1, 0]\n",
    "a5 = [0, 0, 0, 0, 1]\n",
    "\n",
    "## Training set\n",
    "cnt1_tr = 0\n",
    "cnt2_tr = 0\n",
    "cnt3_tr = 0\n",
    "cnt4_tr = 0\n",
    "cnt5_tr = 0        \n",
    "for i in range(len(y_train)):\n",
    "    if (np.array_equal(y_train[i], a1) == True):\n",
    "        cnt1_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a2) == True):\n",
    "        cnt2_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a3) == True):\n",
    "        cnt3_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a4) == True):\n",
    "        cnt4_tr += 1\n",
    "    else:\n",
    "        cnt5_tr += 1\n",
    "table = [['Bubble Class', 'Frequency'], \n",
    "         ['Class 0', cnt1_tr], \n",
    "         ['Class 1', cnt2_tr], \n",
    "         ['Class 2', cnt3_tr], \n",
    "         ['Class 3', cnt4_tr], \n",
    "         ['Class 4', cnt5_tr]]\n",
    "print(tabulate(table, tablefmt=\"grid\"))\n",
    "\n",
    "## Validation set\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "cnt3 = 0\n",
    "cnt4 = 0\n",
    "cnt5 = 0\n",
    "for i in range(len(y_val)):\n",
    "    if (np.array_equal(y_val[i], a1) == True):\n",
    "        cnt1 += 1\n",
    "    elif (np.array_equal(y_val[i], a2) == True):\n",
    "        cnt2 += 1\n",
    "    elif (np.array_equal(y_val[i], a3) == True):\n",
    "        cnt3 += 1\n",
    "    elif (np.array_equal(y_val[i], a4) == True):\n",
    "        cnt4 += 1\n",
    "    else:\n",
    "        cnt5 += 1\n",
    "table = [['Bubble Class', 'Frequency'], \n",
    "         ['Class 0', cnt1], \n",
    "         ['Class 1', cnt2], \n",
    "         ['Class 2', cnt3], \n",
    "         ['Class 3', cnt4], \n",
    "         ['Class 4', cnt5]]\n",
    "print(tabulate(table, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9bc4d",
   "metadata": {},
   "source": [
    "# Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d242c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1),]\n",
    "\n",
    "inp = Input(shape=(MAX_FRAME, 1)) \n",
    "x = Convolution1D(32, kernel_size = 80, padding= 'same')(inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x1 = MaxPool1D(pool_size=3)(x)\n",
    "\n",
    "x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "x = BatchNormalization()(x)\n",
    "x1 = Activation('relu')(x)\n",
    "x1 = Dropout(rate=0.2)(x1)\n",
    "\n",
    "x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "x1 = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling1D()(x1)\n",
    "x = Dense(128)(x)\n",
    "x2 = Dense(128)(x)\n",
    "x_mul = tf.keras.layers.Multiply()([x1, x2])\n",
    "x_add_mul =  tf.keras.layers.Add()([x1, x_mul])\n",
    "\n",
    "x3 = Activation('relu')(x_add_mul)\n",
    "x4 = MaxPool1D(pool_size=3)(x3)\n",
    "\n",
    "x4 = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "\n",
    "x4  = Dropout(rate=0.2)(x4)\n",
    "x4  = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "x11 = BatchNormalization()(x4)\n",
    "x   = GlobalAveragePooling1D()(x11)\n",
    "x   = Dense(128)(x)\n",
    "x21 = Dense(128)(x)\n",
    "x_mul1 = tf.keras.layers.Multiply()([x11, x21])\n",
    "x_add_mul1 =  tf.keras.layers.Add()([x11, x_mul1])\n",
    "x31 = Activation('relu')(x_add_mul1)\n",
    "x41 = MaxPool1D(pool_size=3)(x31)\n",
    "\n",
    "x41 = Convolution1D(256, kernel_size = 3, padding= 'same')(x41)\n",
    "x41 = BatchNormalization()(x41)\n",
    "x41 = Activation('relu')(x41)\n",
    "x41 = Dropout(rate=0.2)(x41)\n",
    "x41 = Convolution1D(256, kernel_size = 3, padding= 'same')(x41)\n",
    "x111 = BatchNormalization()(x41)\n",
    "x = GlobalAveragePooling1D()(x111)\n",
    "x = Dense(256)(x)\n",
    "x211 = Dense(256)(x)\n",
    "x_mul11 = tf.keras.layers.Multiply()([x111, x211])\n",
    "x_add_mul11 =  tf.keras.layers.Add()([x111, x_mul11])\n",
    "\n",
    "x311 = Activation('relu')(x_add_mul11)\n",
    "x411 = MaxPool1D(pool_size=3)(x311)\n",
    "\n",
    "x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "x411 = BatchNormalization()(x411)\n",
    "x411 = Activation('relu')(x411)\n",
    "x411 = Dropout(rate=0.2)(x411)\n",
    "x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "x1111 = BatchNormalization()(x411)\n",
    "x = GlobalAveragePooling1D()(x1111)\n",
    "x = Dense(256)(x)\n",
    "x2111 = Dense(256)(x)\n",
    "x_mul111 = tf.keras.layers.Multiply()([x1111, x2111])\n",
    "x_add_mul111 =  tf.keras.layers.Add()([x1111, x_mul111])\n",
    "\n",
    "x3111 = Activation('relu')(x_add_mul111)\n",
    "x411 = MaxPool1D(pool_size=3)(x3111)\n",
    "\n",
    "\n",
    "x = GlobalMaxPool1D(name = \"Global_Pooling\")(x411)\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "dense_1 = Dense(N_CLASS, activation=activations.softmax)(x)\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f9f3b",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8656e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    validation_data = (x_val, y_val), \n",
    "                    epochs=30, batch_size=64, verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe8adf",
   "metadata": {},
   "source": [
    "# Model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f34885",
   "metadata": {},
   "source": [
    "# Loading the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243553aa",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_real, test_files_real, test_files_real_2, val_files_real= [], [], [], []\n",
    "\n",
    "#Location of real data - Precordial region and subclavian veins\n",
    "DATA_DIR_Train_Real = r'C:\\Users\\Path_to_data\\Real_data\\Percordial_real_data_shuffled_5s_70_10_20\\Train'\n",
    "DATA_DIR_Train_Real_2 = r'C:\\Users\\Path_to_data\\Real_data\\Subclavian_real_data_shuffled_5s_70_10_20\\Train'\n",
    "\n",
    "DATA_DIR_Test_Real = r'C:\\Users\\Path_to_data\\Real_data\\Percordial_real_data_shuffled_5s_70_10_20\\Test'\n",
    "DATA_DIR_Test_Real_2 = r'C:\\Users\\azarang\\Desktop\\Postdoc\\deep_learning_audio_classification\\Real_data\\Subclavian_real_data_shuffled_5s_70_10_20\\Test'\n",
    "\n",
    "DATA_DIR_Validation_Real = r'C:\\Users\\Path_to_data\\Real_data\\Percordial_real_data_shuffled_5s_70_10_20\\Validation'\n",
    "DATA_DIR_Validation_Real_2 = r'C:\\Users\\Path_to_data\\Real_data\\Subclavian_real_data_shuffled_5s_70_10_20\\Validation'\n",
    "\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train_Real)):\n",
    "    train_files_real.append(f)\n",
    "print(len(train_files_real))\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train_Real_2)):\n",
    "    train_files_real.append(f) \n",
    "print(len(train_files_real))\n",
    "    \n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test_Real)):\n",
    "    test_files_real.append(f)\n",
    "print(len(test_files_real))\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test_Real_2)):\n",
    "    test_files_real_2.append(f)\n",
    "print(len(test_files_real_2))\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation_Real)):\n",
    "    val_files_real.append(f)\n",
    "print(len(val_files_real))\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation_Real_2)):\n",
    "    val_files_real.append(f)\n",
    "print(len(val_files_real))\n",
    "\n",
    "L_tr_real = len(train_files_real)\n",
    "# Populating the input and target data - Training\n",
    "x_train_real = np.zeros((L_tr_real, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_train_real = np.zeros((L_tr_real, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_tr_real):\n",
    "    f_train = train_files_real[i]\n",
    "    x_train_real[i] = get_wave(f_train)\n",
    "    label = f_train[len(DATA_DIR_Train_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_train_real[i, label_idx] = 1\n",
    "\n",
    "L_val_real = len(val_files_real)\n",
    "# Populating the input and target data - Validation\n",
    "x_val_real = np.zeros((L_val_real, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_val_real = np.zeros((L_val_real, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_val_real):\n",
    "    f_val = val_files_real[i]\n",
    "    x_val_real[i] = get_wave(f_val)\n",
    "    label = f_val[len(DATA_DIR_Validation_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_val_real[i, label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce0751",
   "metadata": {},
   "source": [
    "# Unfreezing the trained network for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46356ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in reconstructed_model.layers:\n",
    "    if layer.name == 'Global_Pooling':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911fa11",
   "metadata": {},
   "source": [
    "# Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1310757",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=0.0001),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1),]\n",
    "history = reconstructed_model.fit(x_train_real, y_train_real, \n",
    "                    validation_data = (x_val_real, y_val_real), \n",
    "                    epochs=60, batch_size=64, verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de06e89",
   "metadata": {},
   "source": [
    "# Fine-tuned model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538419b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc   = history.history['accuracy']\n",
    "validation_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(training_acc[8:])\n",
    "plt.plot(validation_acc[8:])\n",
    "plt.title('fine-tuning model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "\n",
    "training_loss   = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss[15:])\n",
    "plt.plot(validation_loss[15:])\n",
    "plt.title('fine-tuning model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12a566",
   "metadata": {},
   "source": [
    "# Test set evaluation on fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86520ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "L_test = len(test_files_real)\n",
    "x_test = np.zeros((L_test, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_test = np.zeros((L_test, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_test):\n",
    "    f_test = test_files_real[i]\n",
    "    x_test[i] = get_wave(f_test)\n",
    "    label = f_test[len(DATA_DIR_Test_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_test[i, label_idx] = 1\n",
    "\n",
    "\n",
    "y_pred = reconstructed_model.predict(x_test) #Prediction with test data\n",
    "\n",
    "y_pred_uni8 = np.zeros((L_test, N_CLASS), dtype=np.uint8)\n",
    "\n",
    "for i in range(L_test):\n",
    "    maxElement = np.amax(y_pred[i])\n",
    "    result = np.where(y_pred[i] == np.amax(y_pred[i]))\n",
    "    y_pred_uni8[i, result[0]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f2b2c",
   "metadata": {},
   "source": [
    "# Confusion Matrix of Real Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred_uni8.argmax(axis=1))\n",
    "\n",
    "acc_class_0 = 100*matrix[0,0]/np.sum(matrix[0,:])\n",
    "acc_class_1 = 100*matrix[1,1]/np.sum(matrix[1,:])\n",
    "acc_class_2 = 100*matrix[2,2]/np.sum(matrix[2,:])\n",
    "acc_class_3 = 100*matrix[3,3]/np.sum(matrix[3,:])\n",
    "acc_class_4 = 100*matrix[4,4]/np.sum(matrix[4,:])\n",
    "print('Accuracy of class 0: %' + str(round(acc_class_0,2)))\n",
    "print('Accuracy of class 1: %' + str(acc_class_1))\n",
    "print('Accuracy of class 2: %' + str(acc_class_2))\n",
    "print('Accuracy of class 3: %' + str(acc_class_3))\n",
    "print('Accuracy of class 4: %' + str(acc_class_4))\n",
    "total = (acc_class_0 + acc_class_1 +acc_class_2 +acc_class_3 +acc_class_4)/5\n",
    "print('Total Test Accuracy: %' + str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f126d0",
   "metadata": {},
   "source": [
    "# Binary Classificatin Result (low bubble grade vs. high bubble grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Class\n",
    "low_bubble_class_count = 0\n",
    "total_low_bubble = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        low_bubble_class_count = low_bubble_class_count + matrix[i,j]\n",
    "for i in range(3):\n",
    "    for j in range(len(matrix)):\n",
    "        total_low_bubble = total_low_bubble + matrix[i,j]\n",
    "print('Low bubble class accuracy: %', 100*low_bubble_class_count/total_low_bubble)\n",
    "################################################################################\n",
    "# High Class\n",
    "high_bubble_class_count = 0\n",
    "total_high_bubble = 0\n",
    "for i in [3, 4]:\n",
    "    for j in [3, 4]:\n",
    "        high_bubble_class_count = high_bubble_class_count + matrix[i,j]\n",
    "for i in [3, 4]:\n",
    "    for j in range(len(matrix)):\n",
    "        total_high_bubble = total_high_bubble + matrix[i,j]\n",
    "print('High bubble class accuracy: %', 100*high_bubble_class_count/total_high_bubble)\n",
    "\n",
    "print('Average Accuracy: %', 100*((high_bubble_class_count/total_high_bubble)/2+(low_bubble_class_count/total_low_bubble)/2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
