{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4136633d",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101724dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import random\n",
    "random.seed(42)\n",
    "import math\n",
    "import struct\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "import keras\n",
    "from IPython.core.display import display, HTML\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf53bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebe74a",
   "metadata": {},
   "source": [
    "# Check for available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111cba0",
   "metadata": {},
   "source": [
    "# Visualizing a random test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d94eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"{}/Train/0/syntheticDopplerAudioCombined_0_892425.wav\".format(DATA_DIR)\n",
    "\n",
    "with wave.open(test_file, 'rb') as f:\n",
    "    params = f.getparams()\n",
    "    nchannels, sampwidth, framerate, nframes = params[:4] #Parameters of selected audio file\n",
    "    print(nchannels, sampwidth, framerate, nframes)\n",
    "    strData = f.readframes(nframes) \n",
    "\n",
    "waveData = np.frombuffer(strData, dtype=np.int16)        #Binary to wave conversion\n",
    "waveData_norm = waveData*1.0/(np.max(np.abs(waveData))) #Wave data\n",
    "\n",
    "n = 0.03*np.random.normal(0, 1, waveData_norm.shape)  #Noise to the selected data\n",
    "waveData_norm = n + waveData_norm         #Adding noise to waveform\n",
    "\n",
    "time = np.arange(0,nframes)*(1.0/framerate)\n",
    "plt.plot(time, waveData_norm)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Single channel wavedata\")\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05dd2a",
   "metadata": {},
   "source": [
    "# Organizing the data into train, valiadation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5180041",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAME = 40000 #Length of input frame for 8kHz data\n",
    "LABELS = ['0', '1', '2', '3', '4'] #Target classes for classification task\n",
    "\n",
    "N_CLASS = len(LABELS) #Number of classes for classification task\n",
    "\n",
    "train_files, test_files, val_files = [], [], []\n",
    "\n",
    "#####\n",
    "# \"Path_to_data\" in all data paths must be correctly set \n",
    "#####\n",
    "\n",
    "DATA_DIR_Train = r'PATH-TO-TRAIN-SET'\n",
    "DATA_DIR_Test = r'PATH-TO-TEST-SET'\n",
    "DATA_DIR_Validation = r'PATH-TO-VALIDATION-SET'\n",
    "\n",
    "## Training Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train)):\n",
    "    train_files.append(f)   \n",
    "\n",
    "## Validation Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation)):\n",
    "    val_files.append(f)\n",
    "    \n",
    "## Test Set\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test)):\n",
    "    test_files.append(f) \n",
    "\n",
    "## Suffling the Training and Validation sets to remove bias in dataset\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2652d6a",
   "metadata": {},
   "source": [
    "# Reading and normalizing wave data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378abe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(w):\n",
    "    noise = 0.03*np.random.normal(0, 1, w.shape)\n",
    "    w = noise + w\n",
    "    return w\n",
    "\n",
    "## Functions to read all the data in Train, Validation, and Test sets and do normalization\n",
    "\n",
    "def get_wave_norm(file):\n",
    "    with wave.open(file, 'rb') as f:\n",
    "        params = f.getparams()\n",
    "        nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "        data = f.readframes(nframes)\n",
    "    data = np.frombuffer(data, dtype=np.int16)\n",
    "    data = data/(np.max(np.abs(data)))\n",
    "    return data\n",
    "\n",
    "def get_wave(file):\n",
    "    data = get_wave_norm(file)\n",
    "    wave_data = np.zeros((MAX_FRAME, ))\n",
    "    wave_len = min(MAX_FRAME, data.shape[0])\n",
    "    wave_data[:wave_len] = data[:wave_len]\n",
    "    return np.expand_dims(wave_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13569f3c",
   "metadata": {},
   "source": [
    "# Constructing the input and target: Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the input and target data - Training\n",
    "x_train = np.zeros((L, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_train = np.zeros((L, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L):\n",
    "    f_train = train_files[i]\n",
    "    x_train[i] = get_wave(f_train)\n",
    "    label = f_train[len(DATA_DIR_Train) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_train[i, label_idx] = 1\n",
    "     \n",
    "# Populating the input and target data - Validation\n",
    "L_val = len(val_files)\n",
    "x_val = np.zeros((L_val, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_val = np.zeros((L_val, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_val):\n",
    "    f_val = val_files[i]\n",
    "    x_val[i] = get_wave(f_val)\n",
    "    label = f_val[len(DATA_DIR_Validation) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_val[i, label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2c959",
   "metadata": {},
   "source": [
    "# Dataset balance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [1, 0, 0, 0, 0]\n",
    "a2 = [0, 1, 0, 0, 0]\n",
    "a3 = [0, 0, 1, 0, 0]\n",
    "a4 = [0, 0, 0, 1, 0]\n",
    "a5 = [0, 0, 0, 0, 1]\n",
    "\n",
    "## Training set\n",
    "cnt1_tr = 0\n",
    "cnt2_tr = 0\n",
    "cnt3_tr = 0\n",
    "cnt4_tr = 0\n",
    "cnt5_tr = 0        \n",
    "for i in range(len(y_train)):\n",
    "    if (np.array_equal(y_train[i], a1) == True):\n",
    "        cnt1_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a2) == True):\n",
    "        cnt2_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a3) == True):\n",
    "        cnt3_tr += 1\n",
    "    elif (np.array_equal(y_train[i], a4) == True):\n",
    "        cnt4_tr += 1\n",
    "    else:\n",
    "        cnt5_tr += 1\n",
    "table = [['Bubble Class', 'Frequency'], \n",
    "         ['Class 0', cnt1_tr], \n",
    "         ['Class 1', cnt2_tr], \n",
    "         ['Class 2', cnt3_tr], \n",
    "         ['Class 3', cnt4_tr], \n",
    "         ['Class 4', cnt5_tr]]\n",
    "print(tabulate(table, tablefmt=\"grid\"))\n",
    "\n",
    "## Validation set\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "cnt3 = 0\n",
    "cnt4 = 0\n",
    "cnt5 = 0\n",
    "for i in range(len(y_val)):\n",
    "    if (np.array_equal(y_val[i], a1) == True):\n",
    "        cnt1 += 1\n",
    "    elif (np.array_equal(y_val[i], a2) == True):\n",
    "        cnt2 += 1\n",
    "    elif (np.array_equal(y_val[i], a3) == True):\n",
    "        cnt3 += 1\n",
    "    elif (np.array_equal(y_val[i], a4) == True):\n",
    "        cnt4 += 1\n",
    "    else:\n",
    "        cnt5 += 1\n",
    "table = [['Bubble Class', 'Frequency'], \n",
    "         ['Class 0', cnt1], \n",
    "         ['Class 1', cnt2], \n",
    "         ['Class 2', cnt3], \n",
    "         ['Class 3', cnt4], \n",
    "         ['Class 4', cnt5]]\n",
    "print(tabulate(table, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6c188",
   "metadata": {},
   "source": [
    "# Hyperparameter search with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9036f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Ray Tune\n",
    "\n",
    "from ray import tune\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "import keras_tuner as kt\n",
    "\n",
    "# hp defines the space search for hyperparameters\n",
    "\n",
    "def create_model(hp):\n",
    "        \n",
    "    hp_units = hp.Int('kernel_size', min_value=3, max_value=20, step=1) \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "    hp_dropout = hp.Choice('rate', values=[0.1, 0.2, 0.3])\n",
    "    hp_pool_size = hp.Int('pool_size', min_value=2, max_value=4, step=1)\n",
    "    \n",
    "    inp = Input(shape=(MAX_FRAME, 1)) \n",
    "    x = Convolution1D(32, kernel_size = hp_units, padding= 'same')(inp) #80 Works great\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPool1D(pool_size=hp_pool_size)(x)\n",
    "\n",
    "    x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x1 = Activation('relu')(x)\n",
    "    x1 = Dropout(rate=hp_dropout)(x1)\n",
    "\n",
    "    x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "    x1 = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x1)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x2 = Dense(128, activation = 'sigmoid')(x)\n",
    "    x_mul = tf.keras.layers.Multiply()([x1, x2])\n",
    "    x_add_mul =  tf.keras.layers.Add()([x1, x_mul])\n",
    "\n",
    "    x3 = Activation('relu')(x_add_mul)\n",
    "    x4 = MaxPool1D(pool_size=hp_pool_size)(x3)\n",
    "\n",
    "    x4 = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Activation('relu')(x4)\n",
    "\n",
    "    x4  = Dropout(rate=hp_dropout)(x4)\n",
    "    x4  = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "    x11 = BatchNormalization()(x4)\n",
    "    x   = GlobalAveragePooling1D()(x11)\n",
    "    x   = Dense(128, activation = 'relu')(x)\n",
    "    x21 = Dense(128, activation = 'sigmoid')(x)\n",
    "    x_mul1 = tf.keras.layers.Multiply()([x11, x21])\n",
    "    x_add_mul1 =  tf.keras.layers.Add()([x11, x_mul1])\n",
    "    x31 = Activation('relu')(x_add_mul1)\n",
    "    x41 = MaxPool1D(pool_size=hp_pool_size)(x31)\n",
    "    \n",
    "    x41 = Convolution1D(128, kernel_size = 3, padding= 'same')(x41)\n",
    "    x41 = BatchNormalization()(x41)\n",
    "    x41 = Activation('relu')(x41)\n",
    "    x41 = Dropout(rate=hp_dropout)(x41)\n",
    "    x41 = Convolution1D(128, kernel_size = 3, padding= 'same')(x41)\n",
    "    x111 = BatchNormalization()(x41)\n",
    "    x = GlobalAveragePooling1D()(x111)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x211 = Dense(128, activation = 'sigmoid')(x)\n",
    "    x_mul11 = tf.keras.layers.Multiply()([x111, x211])\n",
    "    x_add_mul11 =  tf.keras.layers.Add()([x111, x_mul11])\n",
    "\n",
    "    x311 = Activation('relu')(x_add_mul11)\n",
    "    x411 = MaxPool1D(pool_size=hp_pool_size)(x311)\n",
    "\n",
    "    x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "    x411 = BatchNormalization()(x411)\n",
    "    x411 = Activation('relu')(x411)\n",
    "    x411 = Dropout(rate=hp_dropout)(x411)\n",
    "    x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "    x1111 = BatchNormalization()(x411)\n",
    "    x = GlobalAveragePooling1D()(x1111)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x2111 = Dense(256, activation = 'sigmoid')(x)\n",
    "    x_mul111 = tf.keras.layers.Multiply()([x1111, x2111])\n",
    "    x_add_mul111 =  tf.keras.layers.Add()([x1111, x_mul111])\n",
    "\n",
    "    x3111 = Activation('relu')(x_add_mul111)\n",
    "    x411 = MaxPool1D(pool_size=4)(x3111)\n",
    "\n",
    "    x = GlobalMaxPool1D(name = \"Global_Pooling\")(x411)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=hp_dropout)(x)\n",
    "\n",
    "    dense_1 = Dense(N_CLASS, activation=activations.softmax)(x)\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate) \n",
    "    \n",
    "\n",
    "    model.compile(loss=tfa.losses.WeightedKappaLoss(num_classes=5), optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(create_model,\n",
    "                     objective='val_accuracy',\n",
    "                                max_epochs=60,\n",
    "                                min_epochs = 10,\n",
    "                                factor=10,\n",
    "                                directory='VGE_model',\n",
    "                                project_name='kt_for_VGE_model')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Keras Tuner loop\n",
    "tuner.search(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[stop_early])\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val))\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828eecbf",
   "metadata": {},
   "source": [
    "# K-fold Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "inputs = x_input_files\n",
    "targets = y_input_files\n",
    "num_folds = 10\n",
    "no_epochs= 80\n",
    "batch_size=64\n",
    "verbosity=1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):    \n",
    "    from tensorflow import keras\n",
    "    from keras import optimizers, losses, activations, models\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    new_model = synthetic_model()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001) \n",
    "    \n",
    "    import tensorflow_addons as tfa\n",
    "\n",
    "    new_model.compile(loss=tfa.losses.WeightedKappaLoss(num_classes=5), optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = new_model.fit(inputs[train], targets[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=verbosity)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = new_model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = new_model.predict(inputs[test])\n",
    "    matrix = confusion_matrix(targets[test].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    print(\"> =============Ordinal Accuracy================ < \")\n",
    "    acc_class_0 = 100*(matrix[0,0]+(3/4)*matrix[0,1]+(2/4)*matrix[0,2]+(1/4)*matrix[0,3])/np.sum(matrix[0,:])\n",
    "    acc_class_1 = 100*(matrix[1,1]+(3/4)*matrix[1,0]+(3/4)*matrix[1,2]+(2/4)*matrix[1,3]+(1/4)*matrix[1,4])/np.sum(matrix[1,:])\n",
    "    acc_class_2 = 100*(matrix[2,2]+(3/4)*matrix[2,1]+(3/4)*matrix[2,3]+(2/4)*matrix[2,4]+(2/4)*matrix[2,0])/np.sum(matrix[2,:])\n",
    "    acc_class_3 = 100*(matrix[3,3]+(3/4)*matrix[3,4]+(3/4)*matrix[3,2]+(2/4)*matrix[3,1]+(1/4)*matrix[3,0])/np.sum(matrix[3,:])\n",
    "    acc_class_4 = 100*(matrix[4,4]+(3/4)*matrix[4,3]+(2/4)*matrix[4,2]+(1/4)*matrix[4,1])/np.sum(matrix[4,:])\n",
    "    print('Accuracy of class 0: %' + str(round(acc_class_0,2)))\n",
    "    print('Accuracy of class 1: %' + str(acc_class_1))\n",
    "    print('Accuracy of class 2: %' + str(acc_class_2))\n",
    "    print('Accuracy of class 3: %' + str(acc_class_3))\n",
    "    print('Accuracy of class 4: %' + str(acc_class_4))\n",
    "    total = (acc_class_0 + acc_class_1 +acc_class_2 +acc_class_3 +acc_class_4)/5\n",
    "    print('Total Test Accuracy: %' + str(total))\n",
    "    print(f'Score for fold {fold_no}: {new_model.metrics_names[0]} of {scores[0]}; {new_model.metrics_names[1]} of {scores[1]*100}%') \n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cedbb",
   "metadata": {},
   "source": [
    "# Ordinal accuracy per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9bc4d",
   "metadata": {},
   "source": [
    "# Final designed network with optimal hyperparameters for VGE grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d242c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1),]\n",
    "\n",
    "inp = Input(shape=(MAX_FRAME, 1)) \n",
    "x = Convolution1D(32, kernel_size = 40, padding= 'same')(inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x1 = MaxPool1D(pool_size=3)(x)\n",
    "\n",
    "x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "x = BatchNormalization()(x)\n",
    "x1 = Activation('relu')(x)\n",
    "x1 = Dropout(rate=0.1)(x1)\n",
    "\n",
    "x = Convolution1D(128, kernel_size = 3, padding= 'same')(x1)\n",
    "x1 = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling1D()(x1)\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x2 = Dense(128, activation = 'sigmoid')(x)\n",
    "x_mul = tf.keras.layers.Multiply()([x1, x2])\n",
    "x_add_mul =  tf.keras.layers.Add()([x1, x_mul])\n",
    "\n",
    "x3 = Activation('relu')(x_add_mul)\n",
    "x4 = MaxPool1D(pool_size=3)(x3)\n",
    "\n",
    "x4 = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "\n",
    "x4  = Dropout(rate=0.1)(x4)\n",
    "x4  = Convolution1D(128, kernel_size = 3, padding= 'same')(x4)\n",
    "x11 = BatchNormalization()(x4)\n",
    "x   = GlobalAveragePooling1D()(x11)\n",
    "x   = Dense(128, activation = 'relu')(x)\n",
    "x21 = Dense(128, activation = 'sigmoid')(x)\n",
    "x_mul1 = tf.keras.layers.Multiply()([x11, x21])\n",
    "x_add_mul1 =  tf.keras.layers.Add()([x11, x_mul1])\n",
    "x31 = Activation('relu')(x_add_mul1)\n",
    "x41 = MaxPool1D(pool_size=3)(x31)\n",
    "\n",
    "x41 = Convolution1D(256, kernel_size = 3, padding= 'same')(x41)\n",
    "x41 = BatchNormalization()(x41)\n",
    "x41 = Activation('relu')(x41)\n",
    "x41 = Dropout(rate=0.1)(x41)\n",
    "x41 = Convolution1D(256, kernel_size = 3, padding= 'same')(x41)\n",
    "x111 = BatchNormalization()(x41)\n",
    "x = GlobalAveragePooling1D()(x111)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x211 = Dense(256, activation = 'sigmoid')(x)\n",
    "x_mul11 = tf.keras.layers.Multiply()([x111, x211])\n",
    "x_add_mul11 =  tf.keras.layers.Add()([x111, x_mul11])\n",
    "\n",
    "x311 = Activation('relu')(x_add_mul11)\n",
    "x411 = MaxPool1D(pool_size=3)(x311)\n",
    "\n",
    "x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "x411 = BatchNormalization()(x411)\n",
    "x411 = Activation('relu')(x411)\n",
    "x411 = Dropout(rate=0.1)(x411)\n",
    "x411 = Convolution1D(256, kernel_size = 3, padding= 'same')(x411)\n",
    "x1111 = BatchNormalization()(x411)\n",
    "x = GlobalAveragePooling1D()(x1111)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x2111 = Dense(256, activation = 'sigmoid')(x)\n",
    "x_mul111 = tf.keras.layers.Multiply()([x1111, x2111])\n",
    "x_add_mul111 =  tf.keras.layers.Add()([x1111, x_mul111])\n",
    "\n",
    "x3111 = Activation('relu')(x_add_mul111)\n",
    "x411 = MaxPool1D(pool_size=3)(x3111)\n",
    "\n",
    "\n",
    "x = GlobalMaxPool1D(name = \"Global_Pooling\")(x411)\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "dense_1 = Dense(N_CLASS, activation=activations.softmax)(x)\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
    "\n",
    "model.compile(loss=tfa.losses.WeightedKappaLoss(num_classes=5), optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f9f3b",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8656e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = syn_model.fit(x_train, y_train, \n",
    "                        validation_data = (x_val, y_val),\n",
    "                        epochs=50, batch_size=64, verbose=1,\n",
    "                        callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe8adf",
   "metadata": {},
   "source": [
    "# Model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f34885",
   "metadata": {},
   "source": [
    "# Loading the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243553aa",
   "metadata": {},
   "source": [
    "# Fine-tuning the model with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_real, test_files_real, test_files_real_2, val_files_real= [], [], [], []\n",
    "\n",
    "#Location of real data - Precordial region and subclavian veins\n",
    "DATA_DIR_Train_Real = r'PATH-TO-PRECORDIAL-REAL-DATA-TRAINING-SET'\n",
    "DATA_DIR_Train_Real_2 = r'PATH-TO-SUBCLAVIAN-REAL-DATA-TRAINING-SET'\n",
    "\n",
    "DATA_DIR_Test_Real = r'PATH-TO-PRECORDIAL-REAL-DATA-TEST-SET'\n",
    "DATA_DIR_Test_Real_2 = r'PATH-TO-SUBCLAVIAN-REAL-DATA-TEST-SET'\n",
    "\n",
    "DATA_DIR_Validation_Real = r'PATH-TO-PRECORDIAL-REAL-DATA-VALIDATION-SET'\n",
    "DATA_DIR_Validation_Real_2 = r'PATH-TO-SUBCLAVIAN-REAL-DATA-VAIDATION-SET'\n",
    "\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train_Real)):\n",
    "    train_files_real.append(f)\n",
    "print(len(train_files_real))\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Train_Real_2)):\n",
    "    train_files_real.append(f) \n",
    "print(len(train_files_real))\n",
    "    \n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test_Real)):\n",
    "    test_files_real.append(f)\n",
    "print(len(test_files_real))\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Test_Real_2)):\n",
    "    test_files_real_2.append(f)\n",
    "print(len(test_files_real_2))\n",
    "\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation_Real)):\n",
    "    val_files_real.append(f)\n",
    "print(len(val_files_real))\n",
    "for f in glob('{}/*/*.wav'.format(DATA_DIR_Validation_Real_2)):\n",
    "    val_files_real.append(f)\n",
    "print(len(val_files_real))\n",
    "\n",
    "L_tr_real = len(train_files_real)\n",
    "# Populating the input and target data - Training\n",
    "x_train_real = np.zeros((L_tr_real, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_train_real = np.zeros((L_tr_real, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_tr_real):\n",
    "    f_train = train_files_real[i]\n",
    "    x_train_real[i] = get_wave(f_train)\n",
    "    label = f_train[len(DATA_DIR_Train_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_train_real[i, label_idx] = 1\n",
    "\n",
    "L_val_real = len(val_files_real)\n",
    "# Populating the input and target data - Validation\n",
    "x_val_real = np.zeros((L_val_real, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_val_real = np.zeros((L_val_real, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_val_real):\n",
    "    f_val = val_files_real[i]\n",
    "    x_val_real[i] = get_wave(f_val)\n",
    "    label = f_val[len(DATA_DIR_Validation_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_val_real[i, label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce0751",
   "metadata": {},
   "source": [
    "# Unfreezing the trained network for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46356ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in reconstructed_model.layers:\n",
    "    if layer.name == 'Global_Pooling':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911fa11",
   "metadata": {},
   "source": [
    "# Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1310757",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=0.0001),\n",
    "             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1),]\n",
    "history = reconstructed_model.fit(x_train_real, y_train_real, \n",
    "                    validation_data = (x_val_real, y_val_real), \n",
    "                    epochs=60, batch_size=64, verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de06e89",
   "metadata": {},
   "source": [
    "# Fine-tuned model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538419b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc   = history.history['accuracy']\n",
    "validation_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(training_acc[8:])\n",
    "plt.plot(validation_acc[8:])\n",
    "plt.title('fine-tuning model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "\n",
    "training_loss   = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss[15:])\n",
    "plt.plot(validation_loss[15:])\n",
    "plt.title('fine-tuning model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12a566",
   "metadata": {},
   "source": [
    "# Test set evaluation on fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86520ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "L_test = len(test_files_real)\n",
    "x_test = np.zeros((L_test, MAX_FRAME, 1), dtype=np.float32)\n",
    "y_test = np.zeros((L_test, N_CLASS), dtype=np.uint8)\n",
    "for i in range(L_test):\n",
    "    f_test = test_files_real[i]\n",
    "    x_test[i] = get_wave(f_test)\n",
    "    label = f_test[len(DATA_DIR_Test_Real) + 1:]\n",
    "    label = label[:label.index('\\\\')]\n",
    "    label_idx = LABELS.index(label[0])\n",
    "    y_test[i, label_idx] = 1\n",
    "\n",
    "\n",
    "y_pred = reconstructed_model.predict(x_test) #Prediction with test data\n",
    "\n",
    "y_pred_uni8 = np.zeros((L_test, N_CLASS), dtype=np.uint8)\n",
    "\n",
    "for i in range(L_test):\n",
    "    maxElement = np.amax(y_pred[i])\n",
    "    result = np.where(y_pred[i] == np.amax(y_pred[i]))\n",
    "    y_pred_uni8[i, result[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f2b2c",
   "metadata": {},
   "source": [
    "# Confusion Matrix of Real Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred_uni8.argmax(axis=1))\n",
    "\n",
    "#Ordinal Accuracy \n",
    "print(\"> =============Ordinal Accuracy================ < \")\n",
    "acc_class_0 = 100*(matrix[0,0]+(3/4)*matrix[0,1]+(2/4)*matrix[0,2]+(1/4)*matrix[0,3])/np.sum(matrix[0,:])\n",
    "acc_class_1 = 100*(matrix[1,1]+(3/4)*matrix[1,0]+(3/4)*matrix[1,2]+(2/4)*matrix[1,3]+(1/4)*matrix[1,4])/np.sum(matrix[1,:])\n",
    "acc_class_2 = 100*(matrix[2,2]+(3/4)*matrix[2,1]+(3/4)*matrix[2,3]+(2/4)*matrix[2,4]+(2/4)*matrix[2,0])/np.sum(matrix[2,:])\n",
    "acc_class_3 = 100*(matrix[3,3]+(3/4)*matrix[3,4]+(3/4)*matrix[3,2]+(2/4)*matrix[3,1]+(1/4)*matrix[3,0])/np.sum(matrix[3,:])\n",
    "acc_class_4 = 100*(matrix[4,4]+(3/4)*matrix[4,3]+(2/4)*matrix[4,2]+(1/4)*matrix[4,1])/np.sum(matrix[4,:])\n",
    "print('Accuracy of class 0: %' + str(round(acc_class_0,2)))\n",
    "print('Accuracy of class 1: %' + str(acc_class_1))\n",
    "print('Accuracy of class 2: %' + str(acc_class_2))\n",
    "print('Accuracy of class 3: %' + str(acc_class_3))\n",
    "print('Accuracy of class 4: %' + str(acc_class_4))\n",
    "total = (acc_class_0 + acc_class_1 +acc_class_2 +acc_class_3 +acc_class_4)/5\n",
    "print('Total Test Accuracy: %' + str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f126d0",
   "metadata": {},
   "source": [
    "# Binary Classificatin Result (low bubble grade vs. high bubble grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Class\n",
    "low_bubble_class_count = 0\n",
    "total_low_bubble = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        low_bubble_class_count = low_bubble_class_count + matrix[i,j]\n",
    "for i in range(3):\n",
    "    for j in range(len(matrix)):\n",
    "        total_low_bubble = total_low_bubble + matrix[i,j]\n",
    "print('Low bubble class accuracy: %', 100*low_bubble_class_count/total_low_bubble)\n",
    "################################################################################\n",
    "# High Class\n",
    "high_bubble_class_count = 0\n",
    "total_high_bubble = 0\n",
    "for i in [3, 4]:\n",
    "    for j in [3, 4]:\n",
    "        high_bubble_class_count = high_bubble_class_count + matrix[i,j]\n",
    "for i in [3, 4]:\n",
    "    for j in range(len(matrix)):\n",
    "        total_high_bubble = total_high_bubble + matrix[i,j]\n",
    "print('High bubble class accuracy: %', 100*high_bubble_class_count/total_high_bubble)\n",
    "\n",
    "print('Average Accuracy: %', 100*((high_bubble_class_count/total_high_bubble)/2+(low_bubble_class_count/total_low_bubble)/2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
